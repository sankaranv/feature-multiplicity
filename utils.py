import json
from sae_lens import SAE, HookedSAETransformer
from functools import partial
import einops
import os
import gc
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import torch.optim as optim
from datasets import load_dataset
from transformers import AutoTokenizer
from transformer_lens.hook_points import (
    HookPoint,
)
import numpy as np
import pandas as pd
from pprint import pprint as pp
from typing import Tuple
from torch import Tensor
from functools import lru_cache
from typing import TypedDict, Optional, Tuple, Union
from tqdm import tqdm
import random
import wandb
import torch.nn.functional as F
import signal


def clear_memory(saes, model):
    for sae in saes:
        for param in sae.parameters():
            param.grad = None
        # for param in sae.mask.parameters():
        #     param.grad = None
    for param in model.parameters():
        param.grad = None
    cleanup_cuda()


def activate_autoreload():
    try:
        ipython = get_ipython()
        if ipython is not None:
            ipython.magic("load_ext autoreload")
            ipython.magic("autoreload 2")
            print("In IPython")
            print("Set autoreload")
        else:
            print("Not in IPython")
    except NameError:
        print("`get_ipython` not available. This script is not running in IPython.")


# Call the function during script initialization
activate_autoreload()


def cleanup_cuda():
    gc.collect()
    torch.cuda.empty_cache()


class SAEMasks(nn.Module):
    def __init__(self, hook_points, masks):
        super().__init__()
        self.hook_points = hook_points  # list of strings
        self.masks = masks

    def forward(self, x, sae_hook_point, mean_ablation=None):
        index = self.hook_points.index(sae_hook_point)
        mask = self.masks[index]
        censored_activations = torch.ones_like(x)
        if mean_ablation is not None:
            censored_activations = censored_activations * mean_ablation
        else:
            censored_activations = censored_activations * 0

        diff_to_x = x - censored_activations
        return censored_activations + diff_to_x * mask

    def print_mask_statistics(self):
        """
        Prints statistics about each binary mask:
          - total number of elements (latents)
          - total number of 'on' latents (mask == 1)
          - average on-latents per token
            * If shape == [latent_dim], there's effectively 1 token
            * If shape == [seq, latent_dim], it's 'sum of on-latents / seq'
        """
        for i, mask in enumerate(self.masks):
            shape = list(mask.shape)
            total_latents = mask.numel()
            total_on = mask.sum().item()  # number of 1's in the mask

            # Average on-latents per token depends on dimensions
            if len(shape) == 1:
                # e.g., shape == [latent_dim]
                avg_on_per_token = total_on  # only one token
            elif len(shape) == 2:
                # e.g., shape == [seq, latent_dim]
                seq_len = shape[0]
                avg_on_per_token = total_on / seq_len if seq_len > 0 else 0
            else:
                # If there's more than 2 dims, adapt as needed;
                # we'll just define "token" as the first dimension.
                seq_len = shape[0]
                avg_on_per_token = total_on / seq_len if seq_len > 0 else 0

            print(f"Statistics for mask '{self.hook_points[i]}':")
            print(f"  - Shape: {shape}")
            print(f"  - Total latents: {total_latents}")
            print(f"  - Latents ON (mask=1): {int(total_on)}")
            print(f"  - Average ON per token: {avg_on_per_token:.4f}\n")

    def save(self, save_dir, file_name="sae_masks.pt"):
        """
        Saves hook_points and masks to a single file (file_name) within save_dir.
        If you want multiple mask sets in the same directory, call save() with
        different file_name values. The directory is created if it does not exist.
        """
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)

        save_path = os.path.join(save_dir, file_name)
        checkpoint = {"hook_points": self.hook_points, "masks": self.masks}
        torch.save(checkpoint, save_path)
        print(f"SAEMasks saved to {save_path}")

    @classmethod
    def load(cls, load_dir, file_name="sae_masks.pt"):
        """
        Loads hook_points and masks from a single file (file_name) within load_dir,
        returning an instance of SAEMasks. If you stored multiple mask sets in the
        directory, specify the file_name to load the correct one.
        """
        load_path = os.path.join(load_dir, file_name)
        if not os.path.isfile(load_path):
            raise FileNotFoundError(f"No saved SAEMasks found at {load_path}")

        checkpoint = torch.load(load_path)
        hook_points = checkpoint["hook_points"]
        masks = checkpoint["masks"]

        instance = cls(hook_points=hook_points, masks=masks)
        print(f"SAEMasks loaded from {load_path}")
        return instance

    def get_num_latents(self):
        num_latents = 0
        for mask in self.masks:
            num_latents += (mask > 0).sum().item()
        return num_latents


class SparseMask(nn.Module):
    def __init__(self, shape, l1, seq_len=None, distinct_l1=0):
        super().__init__()
        if seq_len is not None:
            self.mask = nn.Parameter(torch.ones(seq_len, shape))
        else:
            self.mask = nn.Parameter(torch.ones(shape))
        self.l1 = l1
        self.distinct_l1 = distinct_l1
        self.max_temp = torch.tensor(1000.0)
        self.sparsity_loss = None
        self.ratio_trained = 1
        self.temperature = 1
        self.distinct_sparsity_loss = 0

    def forward(self, x, binary=False, mean_ablation=None):
        if binary:
            # binary mask, 0 if negative, 1 if positive
            binarized = (self.mask > 0).float()
            if mean_ablation is None:
                return x * binarized
            else:
                diff = x - mean_ablation
                return diff * binarized + mean_ablation

        self.temperature = self.max_temp**self.ratio_trained
        mask = torch.sigmoid(self.mask * self.temperature)
        # mask = self.mask
        self.sparsity_loss = torch.abs(mask).sum() * self.l1
        # print("hello", torch.abs(mask).sum())
        # if len(mask.shape) == 2:
        #     self.distinct_sparsity_loss = torch.abs(mask).max(dim=0).values.sum() * self.distinct_l1

        if mean_ablation is None:
            return x * mask
        else:
            diff = x - mean_ablation
            return diff * mask + mean_ablation


class IGMask(nn.Module):
    # igscores is seq x num_sae_latents
    def __init__(self, ig_scores):
        super().__init__()
        self.ig_scores = ig_scores

    def forward(self, x, threshold, mean_ablation=None):
        censored_activations = torch.ones_like(x)
        if mean_ablation != None:
            censored_activations = censored_activations * mean_ablation
        else:
            censored_activations = censored_activations * 0

        mask = (self.ig_scores.abs() > threshold).float()

        diff_to_x = x - censored_activations
        return censored_activations + diff_to_x * mask

    def get_threshold_info(self, threshold):
        mask = (self.ig_scores.abs() > threshold).float()

        total_latents = mask.sum()
        avg_latents_per_tok = mask.sum() / mask.shape[0]
        latents_per_tok = mask.sum(dim=-1)
        return {
            "total_latents": total_latents,
            "avg_latents_per_tok": avg_latents_per_tok,
            "latents_per_tok": latents_per_tok,
        }

    def get_binarized_mask(self, threshold):
        return (self.ig_scores.abs() > threshold).float()


def refresh_class(saes):
    for sae in saes:
        if hasattr(sae, "igmask"):
            sae.igmask = IGMask(sae.igmask.ig_scores)


def produce_ig_binary_masks(saes, threshold=0.01):
    hook_points = []
    masks = []

    for sae in saes:
        hook_point = sae.cfg.hook_name
        mask = sae.igmask.get_binarized_mask(threshold=threshold)
        hook_points.append(hook_point)
        masks.append(mask)

    return SAEMasks(hook_points=hook_points, masks=masks)


def build_sae_hook_fn(
    # Core components
    sae,
    sequence,
    bos_token_id,
    # Masking options
    circuit_mask: Optional[SAEMasks] = None,
    use_mask=False,
    binarize_mask=False,
    mean_mask=False,
    ig_mask_threshold=None,
    # Caching behavior
    cache_sae_grads=False,
    cache_masked_activations=False,
    cache_sae_activations=False,
    # Ablation options
    mean_ablate=False,  # Controls mean ablation of the SAE
    fake_activations=False,  # Controls whether to use fake activations
    calc_error=False,
    use_error=False,
    use_mean_error=False,
):
    # make the mask for the sequence
    mask = torch.ones_like(sequence, dtype=torch.bool)
    # mask[sequence == pad_token_id] = False
    mask[sequence == bos_token_id] = False  # where mask is false, keep original

    def sae_hook(value, hook):
        # print(f"sae {sae.cfg.hook_name} running at layer {hook.layer()}")
        feature_acts = sae.encode(value)
        feature_acts = feature_acts * mask.unsqueeze(-1)
        if fake_activations != False and sae.cfg.hook_layer == fake_activations[0]:
            feature_acts = fake_activations[1]
        if cache_sae_grads:
            raise NotImplementedError("torch is confusing")
            sae.feature_acts = feature_acts.requires_grad_(True)
            sae.feature_acts.retain_grad()

        if cache_sae_activations:
            sae.feature_acts = feature_acts.detach().clone()

        # Learned Binary Masking
        if use_mask:
            if mean_mask:
                # apply the mask, with mean ablations
                feature_acts = sae.mask(
                    feature_acts, binary=binarize_mask, mean_ablation=sae.mean_ablation
                )
            else:
                # apply the mask, without mean ablations
                feature_acts = sae.mask(feature_acts, binary=binarize_mask)

        # IG Masking
        if ig_mask_threshold != None:
            # apply the ig mask
            if mean_mask:
                feature_acts = sae.igmask(
                    feature_acts,
                    threshold=ig_mask_threshold,
                    mean_ablation=sae.mean_ablation,
                )
            else:
                feature_acts = sae.igmask(feature_acts, threshold=ig_mask_threshold)

        if circuit_mask is not None:
            hook_point = sae.cfg.hook_name
            if mean_mask == True:
                feature_acts = circuit_mask(
                    feature_acts, hook_point, mean_ablation=sae.mean_ablation
                )
            else:
                feature_acts = circuit_mask(feature_acts, hook_point)

        if cache_masked_activations:
            sae.feature_acts = feature_acts.detach().clone()
        if mean_ablate:
            feature_acts = sae.mean_ablation

        out = sae.decode(feature_acts)
        # choose out or value based on the mask
        mask_expanded = mask.unsqueeze(-1).expand_as(value)
        updated_value = torch.where(mask_expanded, out, value)
        if calc_error:
            sae.error_term = value - updated_value
            if use_error:
                return updated_value + sae.error_term

        if use_mean_error:
            return updated_value + sae.mean_error
        return updated_value

    return sae_hook


def run_sae_hook_fn(
    model,
    saes,
    sequence,
    # Masking options
    circuit_mask: Optional[SAEMasks] = None,
    use_mask=False,
    binarize_mask=False,
    mean_mask=False,
    ig_mask_threshold=None,
    # Caching behavior
    cache_sae_grads=False,
    cache_masked_activations=False,
    cache_sae_activations=False,
    mean_ablate=False,  # Controls mean ablation of the SAE
    fake_activations=False,  # Controls whether to use fake activations)
    calc_error=False,
    use_error=False,
    use_mean_error=False,
):
    hooks = []
    bos_token_id = model.tokenizer.bos_token_id
    for sae in saes:
        hooks.append(
            (
                sae.cfg.hook_name,
                build_sae_hook_fn(
                    sae,
                    sequence,
                    bos_token_id,
                    cache_sae_grads=cache_sae_grads,
                    circuit_mask=circuit_mask,
                    use_mask=use_mask,
                    binarize_mask=binarize_mask,
                    cache_masked_activations=cache_masked_activations,
                    cache_sae_activations=cache_sae_activations,
                    mean_mask=mean_mask,
                    mean_ablate=mean_ablate,
                    fake_activations=fake_activations,
                    ig_mask_threshold=ig_mask_threshold,
                    calc_error=calc_error,
                    use_error=use_error,
                    use_mean_error=use_mean_error,
                ),
            )
        )

    return model.run_with_hooks(sequence, return_type="logits", fwd_hooks=hooks), saes


def running_mean_tensor(old_mean, new_value, n):
    return old_mean + (new_value - old_mean) / n


def get_sae_means(
    model,
    saes,
    mean_tokens,
    total_batches,
    batch_size,
    per_token_mask=False,
    device="cuda:0",
):
    for sae in saes:
        sae.mean_ablation = torch.zeros(sae.cfg.d_sae).float().to(device)

    with tqdm(total=total_batches * batch_size, desc="Mean Accum Progress") as pbar:
        for i in range(total_batches):
            for j in range(batch_size):
                with torch.no_grad():
                    _, saes = run_sae_hook_fn(
                        model, saes, mean_tokens[i, j], cache_sae_activations=True
                    )

                    for sae in saes:
                        sae.mean_ablation = running_mean_tensor(
                            sae.mean_ablation, sae.feature_acts, i + 1
                        )
                    cleanup_cuda()
                pbar.update(1)

            if i >= total_batches:
                break
    return saes


def get_sae_error_means(
    model,
    saes,
    mean_tokens,
    total_batches,
    batch_size,
    per_token_mask=False,
    device="cuda:0",
):
    for sae in saes:
        sae.mean_error = torch.zeros(sae.cfg.d_in).float().to(device)

    with tqdm(total=total_batches * batch_size, desc="Mean Accum Progress") as pbar:
        for i in range(total_batches):
            for j in range(batch_size):
                with torch.no_grad():
                    _ = run_sae_hook_fn(model, saes, mean_tokens[i, j], calc_error=True)

                    # model.run_with_hooks(
                    # mean_tokens[i, j],
                    # return_type="logits",
                    # fwd_hooks=build_hooks_list(mean_tokens[i, j], cache_sae_activations=True)
                    # )
                    for sae in saes:
                        sae.mean_error = running_mean_tensor(
                            sae.mean_error, sae.error_term, i + 1
                        )
                    cleanup_cuda()
                pbar.update(1)

            if i >= total_batches:
                break
    return saes


class KeyboardInterruptBlocker:
    def __enter__(self):
        # Block SIGINT and store old mask
        self.old_mask = signal.pthread_sigmask(signal.SIG_BLOCK, {signal.SIGINT})

    def __exit__(self, exc_type, exc_value, traceback):
        # Restore old mask (unblock SIGINT)
        signal.pthread_sigmask(signal.SIG_SETMASK, self.old_mask)


class Range:
    def __init__(self, *args):
        # Support for range(start, stop, step) or range(stop)
        self.args = args

        # Validate input like the built-in range does
        if len(self.args) not in {1, 2, 3}:
            raise TypeError(f"Range expected at most 3 arguments, got {len(self.args)}")

        self.range = __builtins__.range(*self.args)  # Create the range object

    def __iter__(self):
        for i in self.range:
            try:
                with KeyboardInterruptBlocker():
                    yield i
            except KeyboardInterrupt:
                print("Keyboard interrupt received. Exiting iteration.")
                break

    def __len__(self):
        return len(self.range)


def logit_diff_fn(logits, clean_labels, corr_labels, token_wise=False):
    clean_logits = logits[torch.arange(logits.shape[0]), -1, clean_labels]
    corr_logits = logits[torch.arange(logits.shape[0]), -1, corr_labels]
    return (
        (clean_logits - corr_logits).mean()
        if not token_wise
        else (clean_logits - corr_logits)
    )


def do_training_run(
    model,
    saes,
    token_dataset,
    labels_dataset,
    corr_labels_dataset,
    sparsity_multiplier,
    example_length=6,
    task="sva/rc_train",
    loss_function="ce",
    per_token_mask=False,
    use_mask=True,
    mean_mask=False,
    portion_of_data=0.5,
    distinct_sparsity_multiplier=0,
    device="cuda:0",
    use_mean_error=False,
    use_error=False,
):

    # def logitfn(tokens):
    #     logits =  model.run_with_hooks(
    #         tokens,
    #         return_type="logits",
    #         fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask)
    #         )
    #     return logits

    def forward_pass(
        model,
        saes,
        batch,
        clean_label_tokens,
        corr_label_tokens,
        ratio_trained=1,
        loss_function="ce",
        use_mask=use_mask,
        mean_mask=mean_mask,
        use_mean_error=use_mean_error,
        use_error=use_error,
    ):
        for sae in saes:
            sae.mask.ratio_trained = ratio_trained
        tokens = batch
        logits, _ = run_sae_hook_fn(
            model,
            saes,
            tokens,
            use_mask=use_mask,
            mean_mask=mean_mask,
            use_mean_error=use_mean_error,
            calc_error=use_error,
            use_error=use_error,
        )
        model_logits, _ = run_sae_hook_fn(
            model,
            saes,
            tokens,
            use_mask=False,
            mean_mask=False,
            use_mean_error=use_mean_error,
        )
        last_token_logits = logits[:, -1, :]
        if loss_function == "ce":
            loss = F.cross_entropy(last_token_logits, clean_label_tokens)
        elif loss_function == "logit_diff":
            fwd_logit_diff = logit_diff_fn(
                logits, clean_label_tokens, corr_label_tokens
            )
            model_logit_diff = logit_diff_fn(
                model_logits, clean_label_tokens, corr_label_tokens
            )
            loss = torch.abs(model_logit_diff - fwd_logit_diff)

        del model_logits, logits
        cleanup_cuda()

        sparsity_loss = 0
        # if per_token_mask:
        distinct_sparsity_loss = 0
        for sae in saes:
            sparsity_loss = sparsity_loss + sae.mask.sparsity_loss
            # if per_token_mask:
            #     distinct_sparsity_loss = distinct_sparsity_loss + sae.mask.distinct_sparsity_loss

        sparsity_loss = sparsity_loss / len(saes)
        distinct_sparsity_loss = distinct_sparsity_loss / len(saes)

        return loss, sparsity_loss, distinct_sparsity_loss

    print("doing a run with sparsity multiplier", sparsity_multiplier)
    all_optimized_params = []
    config = {
        "batch_size": 16,
        "learning_rate": 0.05,
        "total_steps": token_dataset.shape[0] * portion_of_data,
        "sparsity_multiplier": sparsity_multiplier,
    }

    for sae in saes:
        if per_token_mask:
            sae.mask = SparseMask(sae.cfg.d_sae, 1.0, seq_len=example_length).to(device)
        else:
            sae.mask = SparseMask(sae.cfg.d_sae, 1.0).to(device)
        all_optimized_params.extend(list(sae.mask.parameters()))
        sae.mask.max_temp = torch.tensor(200.0)

    wandb.init(project="sae circuits", config=config)
    optimizer = optim.Adam(all_optimized_params, lr=config["learning_rate"])
    total_steps = config["total_steps"]  # *config["batch_size"]

    with tqdm(total=total_steps * 1.1, desc="Training Progress") as pbar:
        for i, (x, y, z) in enumerate(
            zip(token_dataset, labels_dataset, corr_labels_dataset)
        ):
            with KeyboardInterruptBlocker():
                optimizer.zero_grad()

                # Calculate ratio trained
                ratio_trained = i / total_steps * 1.1

                # Update mask ratio for each SAE
                for sae in saes:
                    sae.mask.ratio_trained = ratio_trained

                # Forward pass with updated ratio_trained
                loss, sparsity_loss, distinct_sparsity_loss = forward_pass(
                    model,
                    saes,
                    x,
                    y,
                    z,
                    ratio_trained=ratio_trained,
                    loss_function=loss_function,
                )
                # if per_token_mask:
                #     sparsity_loss = sparsity_loss / example_length

                avg_nonzero_elements = sparsity_loss
                # avg_distinct_nonzero_elements = distinct_sparsity_loss

                sparsity_loss = (
                    sparsity_loss * config["sparsity_multiplier"]
                )  # + distinct_sparsity_loss * distinct_sparsity_multiplier
                total_loss = loss + sparsity_loss
                infodict = {
                    "Step": i,
                    "Progress": ratio_trained,
                    "Avg Nonzero Elements": avg_nonzero_elements.item(),
                    "Task Loss": loss.item(),
                    "Sparsity Loss": sparsity_loss.item(),
                    "temperature": saes[0].mask.temperature,
                }
                # "avg distinct lat/sae":avg_distinct_nonzero_elements.item(),
                wandb.log(infodict)

                # Backward pass and optimizer step
                total_loss.backward()
                optimizer.step()

                # Update tqdm bar with relevant metrics
                pbar.set_postfix(infodict)

                # Update the tqdm progress bar
                pbar.update(1)
                if i >= total_steps * 1.1:
                    break
    wandb.finish()

    optimizer.zero_grad()

    for sae in saes:
        for param in sae.parameters():
            param.grad = None
        for param in sae.mask.parameters():
            param.grad = None

    for param in model.parameters():
        param.grad = None

    torch.cuda.empty_cache()

    ### EVAL ###
    def masked_logit_fn(tokens):
        logits, _ = run_sae_hook_fn(
            model,
            saes,
            tokens,
            use_mask=use_mask,
            mean_mask=mean_mask,
            binarize_mask=True,
            use_mean_error=use_mean_error,
        )

        # model.run_with_hooks(
        # tokens,
        # return_type="logits",
        # fwd_hooks=build_hooks_list(tokens, use_mask=use_mask, mean_mask=mean_mask, binarize_mask=True)
        # )
        return logits

    def eval_ce_loss(batch, labels, logitfn, ratio_trained=10):
        for sae in saes:
            sae.mask.ratio_trained = ratio_trained
        tokens = batch
        logits = logitfn(tokens)
        last_token_logits = logits[:, -1, :]
        loss = F.cross_entropy(last_token_logits, labels)
        return loss

    def eval_logit_diff(
        num_batches, batch, clean_labels, corr_labels, logitfn, ratio_trained=10
    ):
        for sae in saes:
            sae.mask.ratio_trained = ratio_trained
        avg_ld = 0
        avg_model_ld = 0
        for i in range(num_batches):
            tokens = batch[-i]
            logits = logitfn(tokens)
            model_logits = model(tokens)
            ld = logit_diff_fn(logits, clean_labels[-i], corr_labels[-i])
            model_ld = logit_diff_fn(model_logits, clean_labels[-i], corr_labels[-i])
            avg_ld += ld
            avg_model_ld += model_ld
            del logits, model_logits
            cleanup_cuda()
        return (avg_ld / num_batches).item(), (avg_model_ld / num_batches).item()

    with torch.no_grad():
        loss = eval_ce_loss(token_dataset[-1], labels_dataset[-1], masked_logit_fn)
        print("CE loss:", loss)
        cleanup_cuda()
        logit_diff, model_logit_diff = eval_logit_diff(
            10, token_dataset, labels_dataset, corr_labels_dataset, masked_logit_fn
        )
        print("Logit Diff:", logit_diff)
        cleanup_cuda()

    mask_dict = {}

    total_density = 0
    for sae in saes:
        if per_token_mask:
            mask_dict[sae.cfg.hook_name] = torch.where(sae.mask.mask > 0)[1].tolist()
        else:
            mask_dict[sae.cfg.hook_name] = torch.where(sae.mask.mask > 0)[0].tolist()
        # torch.where(sae.mask.mask > 0)[1].tolist()   # rob thinks .view(-1) needed here
        total_density += (sae.mask.mask > 0).sum().item()
    mask_dict["total_density"] = total_density
    mask_dict["avg_density"] = total_density / len(saes)

    if per_token_mask:
        print("total # latents in circuit: ", total_density)
    print("avg density", mask_dict["avg_density"])

    save_path = f"masks/{task}/{loss_function}_{str(sparsity_multiplier)}_run/"
    os.makedirs(save_path, exist_ok=True)
    mask_dict["ce_loss"] = loss.item()
    mask_dict["logit_diff"] = logit_diff
    faithfulness = logit_diff / model_logit_diff
    mask_dict["faithfulness"] = faithfulness

    for idx, sae in enumerate(saes):
        mask_path = f"sae_mask_{idx}.pt"
        torch.save(sae.mask.state_dict(), os.path.join(save_path, mask_path))
        print(f"Saved mask for SAE {idx} to {mask_path}")

    json.dump(
        mask_dict,
        open(os.path.join(save_path, f"{str(sparsity_multiplier)}_run.json"), "w"),
    )
